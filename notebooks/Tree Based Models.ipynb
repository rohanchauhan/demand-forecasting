{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc24a659",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook, the goal was to try tree based models because our dataset contains a lot of categorical variables and tree based models perform well on it.\n",
    "\n",
    "**Models**\n",
    "1. Decision Tree\n",
    "2. Random Forest\n",
    "3. ExtraTreesRegressor\n",
    "4. AdaBoost\n",
    "5. Gradient Boosting\n",
    "6. Voting Regression\n",
    "7. Light Gradient Boosting\n",
    "8. Extreme Gradient Boosting\n",
    "\n",
    "**Workflow**\n",
    "1. Remove outliers\n",
    "2. Encode categorical variables and add date features\n",
    "3. Train Models\n",
    "\n",
    "**Improvements**\n",
    "1. Add Hyperparameter tuning\n",
    "2. RMSE of the model increased after removing outliers from bottom level. Maybe the outlier removal process is too conservative and removing actual data points leading to decrease in RMSE. So, instead I tried removing outliers at top level and got intermediate results between having outliers and not having outliers at all.\n",
    "3. Add visualizations to check the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55804bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c910ff5",
   "metadata": {},
   "source": [
    "### Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe264d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../data/DS_ML Coding Challenge Dataset.xlsx'\n",
    "train_dataset = pd.read_excel(dataset_path, sheet_name='Training Dataset')\n",
    "test_dataset = pd.read_excel(dataset_path, sheet_name='Test Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceeb827",
   "metadata": {},
   "source": [
    "### Remove Outliers from Training Data at Bottom Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a4cd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Renaming columns\\ntrain_dataset.rename(columns={\\'ProductType\\':\\'ProductName\\'}, inplace=True)\\ntrain_dataset.columns = [column_name.replace(\\' \\',\\'\\') for column_name in train_dataset.columns]\\n\\n# Renaming columns\\ntest_dataset.rename(columns={\\'ProductType\\':\\'ProductName\\'}, inplace=True)\\ntest_dataset.columns = [column_name.replace(\\' \\',\\'\\') for column_name in test_dataset.columns]\\n\\n# Creating Combined Column\\ninitial_column = \\'ProductName\\'\\nprev_column = initial_column\\ncolumn_order = [\\'AreaCode\\',\\'Manufacturer\\',\\'SourcingChannel\\',\\'ProductSize\\',\\'ProductType\\']\\n\\nfor column in column_order:\\n    initial_column = initial_column + \\'_\\' + column\\n    train_dataset[initial_column] = train_dataset[prev_column].map(str) + \\'_\\' + train_dataset[column]\\n    prev_column = initial_column\\n\\n# Combined column name\\ncolumn_name = \\'ProductName_AreaCode_Manufacturer_SourcingChannel_ProductSize_ProductType\\'\\n\\n# Grouping Products by CombinedKey\\ngb = train_dataset.groupby([column_name])\\ngroups = [gb.get_group(group_name) for group_name in gb.groups]\\n\\nnew_train_dataset = pd.DataFrame()\\nfor group in groups:\\n    df = group[[column_name,\\'SourcingCost\\',\\'MonthofSourcing\\']].reset_index(drop=True)\\n    \\n    # Removing Outliers using Inter Quartile Range\\n    Q1 = np.percentile(df[\\'SourcingCost\\'], 25, interpolation = \\'midpoint\\') \\n    Q3 = np.percentile(df[\\'SourcingCost\\'], 75, interpolation = \\'midpoint\\') \\n    IQR = Q3 - Q1 \\n    old_shape = df.shape\\n    upper = np.where(df[\\'SourcingCost\\'] > (Q3+1.5*IQR))\\n    lower = np.where(df[\\'SourcingCost\\'] < (Q1-1.5*IQR))\\n    df.drop(upper[0], axis=0, inplace = True)\\n    df.drop(lower[0], axis=0, inplace = True)\\n    #print(\"Removed Outliers: \", old_shape[0]-df.shape[0])\\n    \\n    # Append to new dataframe\\n    new_train_dataset = new_train_dataset.append(df)\\n    \\nnew_train_dataset[column_name.split(\\'_\\')] = new_train_dataset[column_name].str.split(\\'_\\',expand=True)\\nnew_train_dataset.drop([column_name], axis=1, inplace=True)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Renaming columns\n",
    "train_dataset.rename(columns={'ProductType':'ProductName'}, inplace=True)\n",
    "train_dataset.columns = [column_name.replace(' ','') for column_name in train_dataset.columns]\n",
    "\n",
    "# Renaming columns\n",
    "test_dataset.rename(columns={'ProductType':'ProductName'}, inplace=True)\n",
    "test_dataset.columns = [column_name.replace(' ','') for column_name in test_dataset.columns]\n",
    "\n",
    "# Creating Combined Column\n",
    "initial_column = 'ProductName'\n",
    "prev_column = initial_column\n",
    "column_order = ['AreaCode','Manufacturer','SourcingChannel','ProductSize','ProductType']\n",
    "\n",
    "for column in column_order:\n",
    "    initial_column = initial_column + '_' + column\n",
    "    train_dataset[initial_column] = train_dataset[prev_column].map(str) + '_' + train_dataset[column]\n",
    "    prev_column = initial_column\n",
    "\n",
    "# Combined column name\n",
    "column_name = 'ProductName_AreaCode_Manufacturer_SourcingChannel_ProductSize_ProductType'\n",
    "\n",
    "# Grouping Products by CombinedKey\n",
    "gb = train_dataset.groupby([column_name])\n",
    "groups = [gb.get_group(group_name) for group_name in gb.groups]\n",
    "\n",
    "new_train_dataset = pd.DataFrame()\n",
    "for group in groups:\n",
    "    df = group[[column_name,'SourcingCost','MonthofSourcing']].reset_index(drop=True)\n",
    "    \n",
    "    # Removing Outliers using Inter Quartile Range\n",
    "    Q1 = np.percentile(df['SourcingCost'], 25, interpolation = 'midpoint') \n",
    "    Q3 = np.percentile(df['SourcingCost'], 75, interpolation = 'midpoint') \n",
    "    IQR = Q3 - Q1 \n",
    "    old_shape = df.shape\n",
    "    upper = np.where(df['SourcingCost'] > (Q3+1.5*IQR))\n",
    "    lower = np.where(df['SourcingCost'] < (Q1-1.5*IQR))\n",
    "    df.drop(upper[0], axis=0, inplace = True)\n",
    "    df.drop(lower[0], axis=0, inplace = True)\n",
    "    #print(\"Removed Outliers: \", old_shape[0]-df.shape[0])\n",
    "    \n",
    "    # Append to new dataframe\n",
    "    new_train_dataset = new_train_dataset.append(df)\n",
    "    \n",
    "new_train_dataset[column_name.split('_')] = new_train_dataset[column_name].str.split('_',expand=True)\n",
    "new_train_dataset.drop([column_name], axis=1, inplace=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a739f3",
   "metadata": {},
   "source": [
    "### Remove Outliers from Product at Top Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e73052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Renaming columns\n",
    "train_dataset.rename(columns={'ProductType':'ProductName'}, inplace=True)\n",
    "train_dataset.columns = [column_name.replace(' ','') for column_name in train_dataset.columns]\n",
    "\n",
    "# Renaming columns\n",
    "test_dataset.rename(columns={'ProductType':'ProductName'}, inplace=True)\n",
    "test_dataset.columns = [column_name.replace(' ','') for column_name in test_dataset.columns]\n",
    "\n",
    "new_train_dataset = pd.DataFrame()\n",
    "top_level = ['NTM1','NTM2','NTM3']\n",
    "for level in top_level:\n",
    "    df = train_dataset[train_dataset.ProductName==level].copy().reset_index(drop=True)\n",
    "    # Removing Outliers using Inter Quartile Range\n",
    "    Q1 = np.percentile(df['SourcingCost'], 25, interpolation = 'midpoint') \n",
    "    Q3 = np.percentile(df['SourcingCost'], 75, interpolation = 'midpoint') \n",
    "    IQR = Q3 - Q1 \n",
    "    old_shape = df.shape\n",
    "    upper = np.where(df['SourcingCost'] > (Q3+1.5*IQR))\n",
    "    lower = np.where(df['SourcingCost'] < (Q1-1.5*IQR))\n",
    "    df.drop(upper[0], axis=0, inplace = True)\n",
    "    df.drop(lower[0], axis=0, inplace = True)\n",
    "    #print(\"Removed Outliers: \", old_shape[0]-df.shape[0])\n",
    "    \n",
    "    # Append to new dataframe\n",
    "    new_train_dataset = new_train_dataset.append(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58838489",
   "metadata": {},
   "source": [
    "### Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b91e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    '''\n",
    "    Returns X and y after converting categorical variables to one-hot encoding and creating time features\n",
    "    '''\n",
    "    \n",
    "    # Creating time features\n",
    "    dataset['Year'] = pd.DatetimeIndex(dataset['MonthofSourcing']).year\n",
    "    dataset['Month'] = pd.DatetimeIndex(dataset['MonthofSourcing']).month\n",
    "    \n",
    "    # Creating one-hot-encoding for categorical variables\n",
    "    dataset = pd.get_dummies(dataset, columns=['ProductName'], drop_first=True, prefix='ProductName')\n",
    "    dataset = pd.get_dummies(dataset, columns=['Manufacturer'], drop_first=True, prefix='Manufacturer')\n",
    "    dataset = pd.get_dummies(dataset, columns=['AreaCode'], drop_first=True, prefix='AreaCode')\n",
    "    dataset = pd.get_dummies(dataset, columns=['SourcingChannel'], drop_first=True, prefix='SourcingChannel')\n",
    "    dataset = pd.get_dummies(dataset, columns=['ProductSize'], drop_first=True, prefix='ProductSize')\n",
    "    dataset = pd.get_dummies(dataset, columns=['ProductType'], drop_first=True, prefix='ProductType')\n",
    "    \n",
    "    # Creating X and y\n",
    "    X = dataset.drop(['MonthofSourcing','SourcingCost'], axis=1).values\n",
    "    y = dataset['SourcingCost'].values\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c482b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = preprocess_data(new_train_dataset)\n",
    "X_test, y_test = preprocess_data(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e1643",
   "metadata": {},
   "source": [
    "### Tree Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ea9d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training DecisionTreeRegressor\n",
      "Ended Training DecisionTreeRegressor\n",
      "Started Training RandomForestRegressor\n",
      "Ended Training RandomForestRegressor\n",
      "Started Training ExtraTreesRegressor\n",
      "Ended Training ExtraTreesRegressor\n",
      "Started Training AdaBoostRegressor\n",
      "Ended Training AdaBoostRegressor\n",
      "Started Training GradientBoostingRegressor\n",
      "Ended Training GradientBoostingRegressor\n"
     ]
    }
   ],
   "source": [
    "regressors = [DecisionTreeRegressor(), RandomForestRegressor(), ExtraTreesRegressor(), AdaBoostRegressor(),\\\n",
    "              GradientBoostingRegressor()]\n",
    "\n",
    "model_metrics = {}\n",
    "for reg in regressors:\n",
    "    print('Started Training', reg.__class__.__name__)\n",
    "    trained_model = reg.fit(X_train, y_train)\n",
    "    y_pred = trained_model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    model_metrics[reg.__class__.__name__] = [rmse, r2]\n",
    "    print('Ended Training', reg.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5347176c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DecisionTreeRegressor': [34.12919305031901, 0.5706524836546772],\n",
       " 'RandomForestRegressor': [34.1144924600023, 0.5710222728031951],\n",
       " 'ExtraTreesRegressor': [34.024829092084374, 0.5732742799678298],\n",
       " 'AdaBoostRegressor': [38.25561832984171, 0.46055475802549506],\n",
       " 'GradientBoostingRegressor': [33.99121520896884, 0.5741170070894932]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e93399",
   "metadata": {},
   "source": [
    "### Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40076dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [(r.__class__.__name__,r) for r in regressors.copy()[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "831cbebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingRegressor : 31.334522814414328 0.6380879146973715\n"
     ]
    }
   ],
   "source": [
    "vr = VotingRegressor(k)\n",
    "vr.fit(X_train, y_train)\n",
    "y_pred = vr.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(vr.__class__.__name__ , ':',rmse, r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db8615e",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d51b0a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor : 33.16664421051016 0.5945288311357748\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMRegressor()\n",
    "lgbm.fit(X_train, y_train)\n",
    "y_pred = lgbm.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(lgbm.__class__.__name__ , ':',rmse, r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d70ed5",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da56b264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor : 33.956663530227814 0.5749823773601317\n"
     ]
    }
   ],
   "source": [
    "xgbm = XGBRegressor()\n",
    "xgbm.fit(X_train, y_train)\n",
    "y_pred = xgbm.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(xgbm.__class__.__name__ , ':',rmse, r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b3cacd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "I trained 4 classes of models during Experiment:\n",
    "\n",
    "**GROUPED TIME SERIES APPROACH** `DID NOT WORK`\n",
    "1. RMSE of Bottom Up Approach: 376.75748084933656\n",
    "2. Top Down Approach - Not used due to wrong assumption\n",
    "3. Middle Out Approach - Not used due to lack of heirarchy\n",
    "\n",
    "This approach performed best because it was based on the wrong assumption that we had to forecast monthly sum of Sourcing cost. After realising the wrong assumption, I tried to forecast monthly mean which gave this result. Also, the bottom up approach used for Grouped Time Series utilises autoarima to fit the models. After outlier removals, the data at bottom level(i.e. individual product level) became stationary and therefore ARIMA models were too complex for data and performed poorly.\n",
    "\n",
    "**DEEP LEARNING** `NOTEBOOK NOT INCLUDED`\n",
    "\n",
    "LSTM are used to model long range dependencies and hence are suitable for time series forecasting. I faced problems in using this approach due to categorical columns. I read about utilising categorical variables as an auxilary input. However, due to time constraints, wasn't able to do so.\n",
    "\n",
    "**TREE BASED MODELS** `NOTEBOOK INCLUDED`\n",
    "1. RMSE of DecisionTreeRegressor: 34.12919305031901\n",
    "2. RMSE of RandomForestRegressor: 34.1144924600023\n",
    "3. RMSE of ExtraTreesRegressor': 34.024829092084374\n",
    "4. RMSE of AdaBoostRegressor: 38.25561832984171\n",
    "5. RMSE of GradientBoostingRegressor: 33.99121520896884\n",
    "6. RMSE of VotingRegressor : 31.334522814414328\n",
    "7. RMSE of LGBMRegressor : 33.16664421051016\n",
    "8. RMSE of XGBRegressor : 33.956663530227814\n",
    "\n",
    "Tree Based models were chosen as they perform well will categorical variables. \n",
    "Outliers were removed using IQR and categorical variables were one-hot encoded.\n",
    "For outliers removal, there were three choices. Not removing outliers gave the best results. Removing outliers at top product level(i.e. NTM1, NTM2) gave intermediate results and removing outliers at bottom unit level(i.e NTM1_A10_X1_DIRECT_Large_Powder) gave worst results.\n",
    "\n",
    "I would have tuned the hyperparameters of the model but was unable to do so due to time constraints. This would increase the performance(i.e. decrease RMSE)\n",
    "\n",
    "\n",
    "**SIMPLE TIME SERIES BASED MODELS** `NOTEBOOK INCLUDED`\n",
    "\n",
    "Models Used\n",
    "1. RMSE of Simple Moving Average: 37.291999530367285\n",
    "2. RMSE of Exponential Weighted Average: 36.49481290429226\n",
    "3. RMSE of Simple Exponential Smoothing: 37.283745121556244\n",
    "4. RMSE of Additive Exponential Smoothing: 36.173433451689036\n",
    "5. RMSE of Holt's Method: 36.173433451689036\n",
    "\n",
    "Models Not Used due to Complexity or lack of Seasonality\n",
    "1. ARIMA\n",
    "2. SARIMAX\n",
    "3. Holt Winter's method\n",
    "\n",
    "Outliers were removed at the at bottom unit level(i.e NTM1_A10_X1_DIRECT_Large_Powder) using IQR. Then stationarity was tested using Augmented Dickey Fuller Test. After that these simple models were fitted. Trying to fit complex models such as ARIMA using autoarima in pmdarima library result in ARIMA(0,0,0) which means that the model is too complex as the data points were 11 months and even less for some.\n",
    "\n",
    "**FINAL CHOICE**\n",
    "Given all these models, Voting Regressor performed best. But, I would go for a simpler time series model such as Exponential Smoothing because it is easy to understand and diagnose any problems if it happens in future. Given more time, I would like to experiment using LSTMs for the data.\n",
    "\n",
    "**NOTE**\n",
    "1. IQR -> Inter Quartile Range\n",
    "2. All notebooks have more detailed explanations inside them. Please look them through.\n",
    "3. Code would have been better organised given more time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
