{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c35ef4c",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook, the goal was to try traditional time series models on the dataset. \n",
    "\n",
    "**Models**\n",
    "1. Simple Moving Average\n",
    "2. Exponential Weighted Average\n",
    "3. Simple Exponential Smoothing\n",
    "4. Additive Exponential Smoothing\n",
    "5. Holt's method\n",
    "\n",
    "**Models Tried But Not Used**\n",
    "1. ARIMA - For Arima, instead of using ACF and PACF plots, We used pmdarima's **autoarima** to test different models but got ARIMA(0,0,0) as the model is too complex for data. Deciding values based on ACF and PACF plots is prone to human error.\n",
    "2. SARIMAX - Not tried, as there is no seasonal component.\n",
    "3. Holt Winter's method - Not tried, as there is no seasonal component.\n",
    "\n",
    "**Workflow**\n",
    "1. Removed outliers using Inter Quartile range\n",
    "2. Tested for stationarity using Augmented Dickey Fuller Test. We got NaN p-values for some data. This data was visually inspected and found to be stationary and hence the function was modified to call it as stationary.\n",
    "3. Different models used for forecasting.\n",
    "4. Finally RMSE was calculated using Test data.\n",
    "\n",
    "**Improvements**\n",
    "1. Add visualizations to check the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "42f8c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7cf9d",
   "metadata": {},
   "source": [
    "### Augmented Dickey Fuller Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c105777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(df):\n",
    "    \"\"\"\n",
    "    Takes in a time series and returns stationary or non-stationary\n",
    "    \"\"\"\n",
    "    pvalue = adfuller(df,autolag='AIC')[1]\n",
    "    \n",
    "    # print('p-value', pvalue)\n",
    "    # Visual inspection for data with p value of NaN\n",
    "    '''\n",
    "    if(np.isnan(result[1])):\n",
    "        df.plot(figsize=(15,5))\n",
    "    '''\n",
    "    \n",
    "    if (pvalue <= 0.05 or np.isnan(pvalue)):\n",
    "        print(\"Data is stationary\")\n",
    "    else:\n",
    "        print(\"Data is non-stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9230be9f",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7202e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../data/DS_ML Coding Challenge Dataset.xlsx'\n",
    "train_dataset = pd.read_excel(dataset_path, sheet_name='Training Dataset')\n",
    "test_dataset = pd.read_excel(dataset_path, sheet_name='Test Dataset')\n",
    "\n",
    "# Renaming columns\n",
    "train_dataset.rename(columns={'ProductType':'ProductName'}, inplace=True)\n",
    "train_dataset.columns = [column_name.replace(' ','') for column_name in train_dataset.columns]\n",
    "\n",
    "# Renaming columns\n",
    "test_dataset.rename(columns={'ProductType':'ProductName'}, inplace=True)\n",
    "test_dataset.columns = [column_name.replace(' ','') for column_name in test_dataset.columns]\n",
    "\n",
    "# Adding ProductID column to uniquely identify each sourced unit\n",
    "train_dataset['ProductID'] = train_dataset['ProductName'].map(str) + train_dataset['Manufacturer'] + \\\n",
    "                             train_dataset['AreaCode'] + train_dataset['SourcingChannel'] + \\\n",
    "                             train_dataset['ProductSize'] + train_dataset['ProductType']\n",
    "\n",
    "test_dataset['ProductID'] = test_dataset['ProductName'].map(str) + test_dataset['Manufacturer'] + \\\n",
    "                            test_dataset['AreaCode'] + test_dataset['SourcingChannel'] + \\\n",
    "                            test_dataset['ProductSize'] + test_dataset['ProductType'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c58bfd",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7363160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping Products by ProductID\n",
    "gb = train_dataset.groupby(['ProductID'])\n",
    "groups = [gb.get_group(group_name) for group_name in gb.groups]\n",
    "\n",
    "# Creating new dataframe for storing predictions\n",
    "predictions = test_dataset[['ProductID','SourcingCost']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "51066c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in groups:\n",
    "    PID = group.ProductID.unique()[0]\n",
    "    df = group[['SourcingCost','MonthofSourcing']].reset_index(drop=True)\n",
    "    \n",
    "    # Removing Outliers using Inter Quartile Range\n",
    "    Q1 = np.percentile(df['SourcingCost'], 25, interpolation = 'midpoint') \n",
    "    Q3 = np.percentile(df['SourcingCost'], 75, interpolation = 'midpoint') \n",
    "    IQR = Q3 - Q1 \n",
    "    old_shape = df.shape\n",
    "    upper = np.where(df['SourcingCost'] > (Q3+1.5*IQR))\n",
    "    lower = np.where(df['SourcingCost'] < (Q1-1.5*IQR))\n",
    "    df.drop(upper[0], axis=0, inplace = True)\n",
    "    df.drop(lower[0], axis=0, inplace = True)\n",
    "    #print(\"Removed Outliers: \", old_shape[0]-df.shape[0])\n",
    "    \n",
    "    # Using Augmented Dickey Fuller Test for checking stationarity\n",
    "    #adf_test(df['SourcingCost'])\n",
    "    \n",
    "    df = df.groupby('MonthofSourcing').mean()\n",
    "    #df.index.freq='MS'\n",
    "\n",
    "    df['SMA2'] = df['SourcingCost'].rolling(window=2).mean()\n",
    "    df['EWMA3'] = df['SourcingCost'].ewm(span=3,adjust=False).mean()\n",
    "\n",
    "    # Simple Exponential Smoothing Model\n",
    "    sim_exp_model = SimpleExpSmoothing(df['SourcingCost']).fit(optimized=True)\n",
    "    y_pred_sim_exp = sim_exp_model.forecast(1).values[0]\n",
    "\n",
    "    # Additive Exponential Smoothing Model\n",
    "    exp_add_model = ExponentialSmoothing(df['SourcingCost'],trend='add').fit(optimized=True)\n",
    "    y_pred_exp_add = exp_add_model.forecast(1).values[0]\n",
    "\n",
    "    # Holt's Model\n",
    "    holt_model = Holt(df['SourcingCost']).fit(optimized=True)\n",
    "    y_pred_holt = holt_model.forecast(1).values[0]\n",
    "\n",
    "    # Simple Moving Average\n",
    "    y_pred_sma = df.loc[df.index[-1], \"SMA2\"]\n",
    "    \n",
    "    # Exponential Weighted Average\n",
    "    y_pred_ewma = df.loc[df.index[-1], \"EWMA3\"]\n",
    "\n",
    "    # Adding to predictions\n",
    "    predictions.loc[predictions.ProductID==PID,'SMA2']=y_pred_sma\n",
    "    predictions.loc[predictions.ProductID==PID,'EWMA3']=y_pred_ewma\n",
    "    predictions.loc[predictions.ProductID==PID,'SES']=y_pred_sim_exp\n",
    "    predictions.loc[predictions.ProductID==PID,'ESA']=y_pred_exp_add\n",
    "    predictions.loc[predictions.ProductID==PID,'Holt']=y_pred_holt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e6b12",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "0875aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating RMSE for each Model\n",
    "rmse_sma2 = np.sqrt(mean_squared_error(predictions['SourcingCost'].values,predictions['SMA2'].values))\n",
    "rmse_ewma3 = np.sqrt(mean_squared_error(predictions['SourcingCost'].values,predictions['EWMA3'].values))\n",
    "rmse_ses = np.sqrt(mean_squared_error(predictions['SourcingCost'].values,predictions['SES'].values))\n",
    "rmse_esa = np.sqrt(mean_squared_error(predictions['SourcingCost'].values,predictions['ESA'].values))\n",
    "rmse_holt = np.sqrt(mean_squared_error(predictions['SourcingCost'].values,predictions['Holt'].values))\n",
    "\n",
    "# Printing RMSE\n",
    "print(\"RMSE of Simple Moving Average:\",rmse_sma2)\n",
    "print(\"RMSE of Exponential Weighted Average:\",rmse_ewma3)\n",
    "print(\"RMSE of Simple Exponential Smoothing:\",rmse_ses)\n",
    "print(\"RMSE of Additive Exponential Smoothing:\",rmse_esa)\n",
    "print(\"RMSE of Holt's Method:\",rmse_holt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
