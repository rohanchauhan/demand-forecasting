{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc24a659",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook, the goal was to try tree based models because our dataset contains a lot of categorical variables and tree based models perform well on it.\n",
    "\n",
    "**Models**\n",
    "1. Decision Tree\n",
    "2. Random Forest\n",
    "3. ExtraTreesRegressor\n",
    "4. AdaBoost\n",
    "5. Gradient Boosting\n",
    "6. Voting Regression\n",
    "7. Light Gradient Boosting\n",
    "8. Extreme Gradient Boosting\n",
    "\n",
    "**Workflow**\n",
    "1. Remove outliers\n",
    "2. Encode categorical variables and add date features\n",
    "3. Train Models\n",
    "\n",
    "**Improvements**\n",
    "1. Add Hyperparameter tuning\n",
    "2. RMSE of the model increased after removing outliers. Maybe the outlier removal process is too conservative and removing actual data points leading to decrease in RMSE. Try or play with more outlier removal methods.\n",
    "3. Add visualizations to check the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55804bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "#from lightgbm import LGBMRegressor\n",
    "#from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c910ff5",
   "metadata": {},
   "source": [
    "### Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe264d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../data/DS_ML Coding Challenge Dataset.xlsx'\n",
    "train_dataset = pd.read_excel(dataset_path, sheet_name='Training Dataset')\n",
    "test_dataset = pd.read_excel(dataset_path, sheet_name='Test Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceeb827",
   "metadata": {},
   "source": [
    "### Remove Outliers from Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5a4cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "train_dataset.rename(columns={'ProductType':'ProductName'}, inplace=True)\n",
    "train_dataset.columns = [column_name.replace(' ','') for column_name in train_dataset.columns]\n",
    "\n",
    "# Renaming columns\n",
    "test_dataset.rename(columns={'ProductType':'ProductName'}, inplace=True)\n",
    "test_dataset.columns = [column_name.replace(' ','') for column_name in test_dataset.columns]\n",
    "\n",
    "# Creating Combined Column\n",
    "initial_column = 'ProductName'\n",
    "prev_column = initial_column\n",
    "column_order = ['AreaCode','Manufacturer','SourcingChannel','ProductSize','ProductType']\n",
    "\n",
    "for column in column_order:\n",
    "    initial_column = initial_column + '_' + column\n",
    "    train_dataset[initial_column] = train_dataset[prev_column].map(str) + '_' + train_dataset[column]\n",
    "    prev_column = initial_column\n",
    "\n",
    "# Combined column name\n",
    "column_name = 'ProductName_AreaCode_Manufacturer_SourcingChannel_ProductSize_ProductType'\n",
    "\n",
    "# Grouping Products by CombinedKey\n",
    "gb = train_dataset.groupby([column_name])\n",
    "groups = [gb.get_group(group_name) for group_name in gb.groups]\n",
    "\n",
    "new_train_dataset = pd.DataFrame()\n",
    "for group in groups:\n",
    "    df = group[[column_name,'SourcingCost','MonthofSourcing']].reset_index(drop=True)\n",
    "    \n",
    "    # Removing Outliers using Inter Quartile Range\n",
    "    Q1 = np.percentile(df['SourcingCost'], 25, interpolation = 'midpoint') \n",
    "    Q3 = np.percentile(df['SourcingCost'], 75, interpolation = 'midpoint') \n",
    "    IQR = Q3 - Q1 \n",
    "    old_shape = df.shape\n",
    "    upper = np.where(df['SourcingCost'] > (Q3+1.5*IQR))\n",
    "    lower = np.where(df['SourcingCost'] < (Q1-1.5*IQR))\n",
    "    df.drop(upper[0], axis=0, inplace = True)\n",
    "    df.drop(lower[0], axis=0, inplace = True)\n",
    "    #print(\"Removed Outliers: \", old_shape[0]-df.shape[0])\n",
    "    \n",
    "    # Append to new dataframe\n",
    "    new_train_dataset = new_train_dataset.append(df)\n",
    "    \n",
    "new_train_dataset[column_name.split('_')] = new_train_dataset[column_name].str.split('_',expand=True)\n",
    "new_train_dataset.drop([column_name], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58838489",
   "metadata": {},
   "source": [
    "### Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39b91e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    '''\n",
    "    Returns X and y after converting categorical variables to one-hot encoding and creating time features\n",
    "    '''\n",
    "    \n",
    "    # Creating time features\n",
    "    dataset['Year'] = pd.DatetimeIndex(dataset['MonthofSourcing']).year\n",
    "    dataset['Month'] = pd.DatetimeIndex(dataset['MonthofSourcing']).month\n",
    "    \n",
    "    # Creating one-hot-encoding for categorical variables\n",
    "    dataset = pd.get_dummies(dataset, columns=['ProductName'], drop_first=True, prefix='ProductName')\n",
    "    dataset = pd.get_dummies(dataset, columns=['Manufacturer'], drop_first=True, prefix='Manufacturer')\n",
    "    dataset = pd.get_dummies(dataset, columns=['AreaCode'], drop_first=True, prefix='AreaCode')\n",
    "    dataset = pd.get_dummies(dataset, columns=['SourcingChannel'], drop_first=True, prefix='SourcingChannel')\n",
    "    dataset = pd.get_dummies(dataset, columns=['ProductSize'], drop_first=True, prefix='ProductSize')\n",
    "    dataset = pd.get_dummies(dataset, columns=['ProductType'], drop_first=True, prefix='ProductType')\n",
    "    \n",
    "    # Creating X and y\n",
    "    X = dataset.drop(['MonthofSourcing','SourcingCost'], axis=1).values\n",
    "    y = dataset['SourcingCost'].values\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c482b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = preprocess_data(new_train_dataset)\n",
    "X_test, y_test = preprocess_data(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e1643",
   "metadata": {},
   "source": [
    "### Tree Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0ea9d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training DecisionTreeRegressor\n",
      "Ended Training DecisionTreeRegressor\n",
      "Started Training RandomForestRegressor\n",
      "Ended Training RandomForestRegressor\n",
      "Started Training ExtraTreesRegressor\n",
      "Ended Training ExtraTreesRegressor\n",
      "Started Training AdaBoostRegressor\n",
      "Ended Training AdaBoostRegressor\n",
      "Started Training GradientBoostingRegressor\n",
      "Ended Training GradientBoostingRegressor\n"
     ]
    }
   ],
   "source": [
    "regressors = [DecisionTreeRegressor(), RandomForestRegressor(), ExtraTreesRegressor(), AdaBoostRegressor(),\\\n",
    "              GradientBoostingRegressor()]\n",
    "\n",
    "model_metrics = {}\n",
    "for reg in regressors:\n",
    "    print('Started Training', reg.__class__.__name__)\n",
    "    trained_model = reg.fit(X_train, y_train)\n",
    "    y_pred = trained_model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    model_metrics[reg.__class__.__name__] = [rmse, r2]\n",
    "    print('Ended Training', reg.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5347176c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DecisionTreeRegressor': [36.740695189343626, 0.5024329024702691],\n",
       " 'RandomForestRegressor': [36.73549639053813, 0.5025737037374551],\n",
       " 'ExtraTreesRegressor': [36.64166663085347, 0.5051115098302945],\n",
       " 'AdaBoostRegressor': [39.74390240779491, 0.41776550698904313],\n",
       " 'GradientBoostingRegressor': [33.47924402152275, 0.5868495779485705]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e93399",
   "metadata": {},
   "source": [
    "### Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40076dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [(r.__class__.__name__,r) for r in regressors.copy()[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "831cbebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingRegressor : 31.568090657221372 0.6326724135199246\n"
     ]
    }
   ],
   "source": [
    "vr = VotingRegressor(k)\n",
    "vr.fit(X_train, y_train)\n",
    "y_pred = vr.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(vr.__class__.__name__ , ':',rmse, r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db8615e",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b0a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMRegressor()\n",
    "lgbm.fit(X_train, y_train)\n",
    "y_pred = lgbm.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(lgbm.__class__.__name__ , ':',rmse, r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d70ed5",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da56b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbm = XGBRegressor()\n",
    "xgbm.fit(X_train, y_train)\n",
    "y_pred = xgbm.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(xgbm.__class__.__name__ , ':',rmse, r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
