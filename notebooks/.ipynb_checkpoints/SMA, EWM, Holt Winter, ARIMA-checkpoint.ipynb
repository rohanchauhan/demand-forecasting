{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c35ef4c",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook, the goal was to try traditional time series models on the dataset. \n",
    "\n",
    "**Models**\n",
    "1. Simple Moving Average\n",
    "2. Exponential Weighted Average\n",
    "3. Simple Exponential Smoothing\n",
    "4. Additive Exponential Smoothing\n",
    "5. Holt's method\n",
    "\n",
    "**Models Tried But Not Used**\n",
    "1. ARIMA - For Arima, instead of using ACF and PACF plots, We used pmdarima's **autoarima** to test different models but got ARIMA(0,0,0) as the model is too complex for data. Deciding values based on ACF and PACF plots is prone to human error.\n",
    "2. SARIMAX - Not tried, as there is no seasonal component.\n",
    "3. Holt Winter's method - Not tried, as there is no seasonal component.\n",
    "\n",
    "**Workflow**\n",
    "1. Removed outliers using Inter Quartile range\n",
    "2. Tested for stationarity using Augmented Dickey Fuller Test. We got NaN p-values for some data. This data was visually inspected and found to be stationary and hence the function was modified to call it as stationary.\n",
    "3. Different models used for forecasting.\n",
    "4. Finally RMSE was calculated using Test data.\n",
    "\n",
    "**Improvements**\n",
    "1. Add visualizations to check the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42f8c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7cf9d",
   "metadata": {},
   "source": [
    "### Augmented Dickey Fuller Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c105777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(df):\n",
    "    \"\"\"\n",
    "    Takes in a time series and returns stationary or non-stationary\n",
    "    \"\"\"\n",
    "    pvalue = adfuller(df,autolag='AIC')[1]\n",
    "    \n",
    "    # print('p-value', pvalue)\n",
    "    # Visual inspection for data with p value of NaN\n",
    "    '''\n",
    "    if(np.isnan(result[1])):\n",
    "        df.plot(figsize=(15,5))\n",
    "    '''\n",
    "    \n",
    "    if (pvalue <= 0.05 or np.isnan(pvalue)):\n",
    "        print(\"Data is stationary\")\n",
    "    else:\n",
    "        print(\"Data is non-stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9230be9f",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7202e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../data/DS_ML Coding Challenge Dataset.xlsx'\n",
    "train_dataset = pd.read_excel(dataset_path, sheet_name='Training Dataset')\n",
    "test_dataset = pd.read_excel(dataset_path, sheet_name='Test Dataset')\n",
    "\n",
    "# Renaming columns\n",
    "train_dataset.rename(columns={'ProductType':'ProductName'}, inplace=True)\n",
    "train_dataset.columns = [column_name.replace(' ','') for column_name in train_dataset.columns]\n",
    "\n",
    "# Renaming columns\n",
    "test_dataset.rename(columns={'ProductType':'ProductName'}, inplace=True)\n",
    "test_dataset.columns = [column_name.replace(' ','') for column_name in test_dataset.columns]\n",
    "\n",
    "# Adding ProductID column to uniquely identify each sourced unit\n",
    "train_dataset['ProductID'] = train_dataset['ProductName'].map(str) + train_dataset['Manufacturer'] + \\\n",
    "                             train_dataset['AreaCode'] + train_dataset['SourcingChannel'] + \\\n",
    "                             train_dataset['ProductSize'] + train_dataset['ProductType']\n",
    "\n",
    "test_dataset['ProductID'] = test_dataset['ProductName'].map(str) + test_dataset['Manufacturer'] + \\\n",
    "                            test_dataset['AreaCode'] + test_dataset['SourcingChannel'] + \\\n",
    "                            test_dataset['ProductSize'] + test_dataset['ProductType'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c58bfd",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7363160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping Products by ProductID\n",
    "gb = train_dataset.groupby(['ProductID'])\n",
    "groups = [gb.get_group(group_name) for group_name in gb.groups]\n",
    "\n",
    "# Creating new dataframe for storing predictions\n",
    "predictions = test_dataset[['ProductID','SourcingCost']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51066c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in groups:\n",
    "    PID = group.ProductID.unique()[0]\n",
    "    df = group[['SourcingCost','MonthofSourcing']].reset_index(drop=True)\n",
    "    \n",
    "    # Removing Outliers using Inter Quartile Range\n",
    "    Q1 = np.percentile(df['SourcingCost'], 25, interpolation = 'midpoint') \n",
    "    Q3 = np.percentile(df['SourcingCost'], 75, interpolation = 'midpoint') \n",
    "    IQR = Q3 - Q1 \n",
    "    old_shape = df.shape\n",
    "    upper = np.where(df['SourcingCost'] > (Q3+1.5*IQR))\n",
    "    lower = np.where(df['SourcingCost'] < (Q1-1.5*IQR))\n",
    "    df.drop(upper[0], axis=0, inplace = True)\n",
    "    df.drop(lower[0], axis=0, inplace = True)\n",
    "    #print(\"Removed Outliers: \", old_shape[0]-df.shape[0])\n",
    "    \n",
    "    # Using Augmented Dickey Fuller Test for checking stationarity\n",
    "    #adf_test(df['SourcingCost'])\n",
    "    \n",
    "    df = df.groupby('MonthofSourcing').mean()\n",
    "    #df.index.freq='MS'\n",
    "\n",
    "    df['SMA2'] = df['SourcingCost'].rolling(window=2).mean()\n",
    "    df['EWMA3'] = df['SourcingCost'].ewm(span=3,adjust=False).mean()\n",
    "\n",
    "    # Simple Exponential Smoothing Model\n",
    "    sim_exp_model = SimpleExpSmoothing(df['SourcingCost']).fit(optimized=True)\n",
    "    y_pred_sim_exp = sim_exp_model.forecast(1).values[0]\n",
    "\n",
    "    # Additive Exponential Smoothing Model\n",
    "    exp_add_model = ExponentialSmoothing(df['SourcingCost'],trend='add').fit(optimized=True)\n",
    "    y_pred_exp_add = exp_add_model.forecast(1).values[0]\n",
    "\n",
    "    # Holt's Model\n",
    "    holt_model = Holt(df['SourcingCost']).fit(optimized=True)\n",
    "    y_pred_holt = holt_model.forecast(1).values[0]\n",
    "\n",
    "    # Simple Moving Average\n",
    "    y_pred_sma = df.loc[df.index[-1], \"SMA2\"]\n",
    "    \n",
    "    # Exponential Weighted Average\n",
    "    y_pred_ewma = df.loc[df.index[-1], \"EWMA3\"]\n",
    "\n",
    "    # Adding to predictions\n",
    "    predictions.loc[predictions.ProductID==PID,'SMA2']=y_pred_sma\n",
    "    predictions.loc[predictions.ProductID==PID,'EWMA3']=y_pred_ewma\n",
    "    predictions.loc[predictions.ProductID==PID,'SES']=y_pred_sim_exp\n",
    "    predictions.loc[predictions.ProductID==PID,'ESA']=y_pred_exp_add\n",
    "    predictions.loc[predictions.ProductID==PID,'Holt']=y_pred_holt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e6b12",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0875aa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of Simple Moving Average: 37.291999530367285\n",
      "RMSE of Exponential Weighted Average: 36.49481290429226\n",
      "RMSE of Simple Exponential Smoothing: 37.283745121556244\n",
      "RMSE of Additive Exponential Smoothing: 36.173433451689036\n",
      "RMSE of Holt's Method: 36.173433451689036\n"
     ]
    }
   ],
   "source": [
    "# Calculating RMSE for each Model\n",
    "rmse_sma2 = np.sqrt(mean_squared_error(predictions['SourcingCost'].values,predictions['SMA2'].values))\n",
    "rmse_ewma3 = np.sqrt(mean_squared_error(predictions['SourcingCost'].values,predictions['EWMA3'].values))\n",
    "rmse_ses = np.sqrt(mean_squared_error(predictions['SourcingCost'].values,predictions['SES'].values))\n",
    "rmse_esa = np.sqrt(mean_squared_error(predictions['SourcingCost'].values,predictions['ESA'].values))\n",
    "rmse_holt = np.sqrt(mean_squared_error(predictions['SourcingCost'].values,predictions['Holt'].values))\n",
    "\n",
    "# Printing RMSE\n",
    "print(\"RMSE of Simple Moving Average:\",rmse_sma2)\n",
    "print(\"RMSE of Exponential Weighted Average:\",rmse_ewma3)\n",
    "print(\"RMSE of Simple Exponential Smoothing:\",rmse_ses)\n",
    "print(\"RMSE of Additive Exponential Smoothing:\",rmse_esa)\n",
    "print(\"RMSE of Holt's Method:\",rmse_holt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4154d188",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "I trained 4 classes of models during Experiment:\n",
    "\n",
    "**GROUPED TIME SERIES APPROACH** `DID NOT WORK`\n",
    "1. RMSE of Bottom Up Approach: 376.75748084933656\n",
    "2. Top Down Approach - Not used due to wrong assumption\n",
    "3. Middle Out Approach - Not used due to lack of heirarchy\n",
    "\n",
    "This approach performed best because it was based on the wrong assumption that we had to forecast monthly sum of Sourcing cost. After realising the wrong assumption, I tried to forecast monthly mean which gave this result. Also, the bottom up approach used for Grouped Time Series utilises autoarima to fit the models. After outlier removals, the data at bottom level(i.e. individual product level) became stationary and therefore ARIMA models were too complex for data and performed poorly.\n",
    "\n",
    "**DEEP LEARNING** `NOTEBOOK NOT INCLUDED`\n",
    "\n",
    "LSTM are used to model long range dependencies and hence are suitable for time series forecasting. I faced problems in using this approach due to categorical columns. I read about utilising categorical variables as an auxilary input. However, due to time constraints, wasn't able to do so.\n",
    "\n",
    "**TREE BASED MODELS** `NOTEBOOK INCLUDED`\n",
    "1. RMSE of DecisionTreeRegressor: 34.12919305031901\n",
    "2. RMSE of RandomForestRegressor: 34.1144924600023\n",
    "3. RMSE of ExtraTreesRegressor': 34.024829092084374\n",
    "4. RMSE of AdaBoostRegressor: 38.25561832984171\n",
    "5. RMSE of GradientBoostingRegressor: 33.99121520896884\n",
    "6. RMSE of VotingRegressor : 31.334522814414328\n",
    "7. RMSE of LGBMRegressor : 33.16664421051016\n",
    "8. RMSE of XGBRegressor : 33.956663530227814\n",
    "\n",
    "Tree Based models were chosen as they perform well will categorical variables. \n",
    "Outliers were removed using IQR and categorical variables were one-hot encoded.\n",
    "For outliers removal, there were three choices. Not removing outliers gave the best results. Removing outliers at top product level(i.e. NTM1, NTM2) gave intermediate results and removing outliers at bottom unit level(i.e NTM1_A10_X1_DIRECT_Large_Powder) gave worst results.\n",
    "\n",
    "I would have tuned the hyperparameters of the model but was unable to do so due to time constraints. This would increase the performance(i.e. decrease RMSE)\n",
    "\n",
    "\n",
    "**SIMPLE TIME SERIES BASED MODELS** `NOTEBOOK INCLUDED`\n",
    "\n",
    "Models Used\n",
    "1. RMSE of Simple Moving Average: 37.291999530367285\n",
    "2. RMSE of Exponential Weighted Average: 36.49481290429226\n",
    "3. RMSE of Simple Exponential Smoothing: 37.283745121556244\n",
    "4. RMSE of Additive Exponential Smoothing: 36.173433451689036\n",
    "5. RMSE of Holt's Method: 36.173433451689036\n",
    "\n",
    "Models Not Used due to Complexity or lack of Seasonality\n",
    "1. ARIMA\n",
    "2. SARIMAX\n",
    "3. Holt Winter's method\n",
    "\n",
    "Outliers were removed at the at bottom unit level(i.e NTM1_A10_X1_DIRECT_Large_Powder) using IQR. Then stationarity was tested using Augmented Dickey Fuller Test. After that these simple models were fitted. Trying to fit complex models such as ARIMA using autoarima in pmdarima library result in ARIMA(0,0,0) which means that the model is too complex as the data points were 11 months and even less for some.\n",
    "\n",
    "**FINAL CHOICE**\n",
    "Given all these models, Voting Regressor performed best. But, I would go for a simpler time series model such as Exponential Smoothing because it is easy to understand and diagnose any problems if it happens in future. Given more time, I would like to experiment using LSTMs for the data.\n",
    "\n",
    "**NOTE**\n",
    "1. IQR -> Inter Quartile Range\n",
    "2. All notebooks have more detailed explanations inside them. Please look them through.\n",
    "3. Code would have been better organised given more time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
